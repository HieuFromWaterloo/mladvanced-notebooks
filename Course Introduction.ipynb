{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ublogo.png\"/>\n",
    "\n",
    "### CSE610 - Bayesian Non-parametric Machine Learning\n",
    "\n",
    "  - Lecture Notes\n",
    "  - Instructor - Varun Chandola\n",
    "  - Term - Fall 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "The objective of this notebook is to provide a gentle introduction to Machine Learning and to the course in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Machine Learning\n",
    "> Use what we have to infer what we do not\n",
    "\n",
    "There are many types of ML problems, but most can be thought of as the following task:\n",
    "- Given ${\\bf x}$ find $y$\n",
    "  * ${\\bf x}$ refers to the observed part of the data. Other names include - inputs, independent covariates, etc. Here we have denoted it as a mathematical vector, i.e., ${\\bf x} \\in \\mathbb{R}^D$, though this might not always be the case.\n",
    "  * $y$ refers to the part that we want the ML method to estimate/infer. Depending on the problem, $y$ could mean:\n",
    "    - A categorical target (classification)\n",
    "    - A continuous value or vector of values (univariate/multivariate regression)\n",
    "    - A membership indicator (clustering)\n",
    "    - A vector representation (representation learning, dimensionality reduction)\n",
    "    - A state of a stochastic system (reinforcement learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised ML\n",
    "We will start with one flavor of machine learning - *supervised learning*, where we use a collection of $\\langle {\\bf x},y\\rangle$ pairs (referred to as **training data**) to learn an ML model that can be used for the aforementioned inference. The training data provides supervision, hence the name.\n",
    "\n",
    "Following discussions will be based on supervised ML, even though many of the concepts translate to other ML flavors (unsupervised learning, reinforcement learning, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional vs. Probabilistic Methods\n",
    "**Functional models** learn a mathematical relationship between ${\\bf x}$ and $y$, i.e.,\n",
    "$$\n",
    "y = f_{\\Theta}({\\bf x})\n",
    "$$\n",
    "The function $f$ is parameterized by a set of parameters ($\\Theta$) that are learned using the training data.\n",
    "\n",
    "**Probabilistic models** estimate the probability (or density) $p(y \\vert {\\bf x})$. Some models directly estimate the conditional probability (discriminative models), while others treat ${\\bf x}$ also as a random variable, model the joint distribution $(y,{\\bf x})$ and then estimate $p(y \\vert {\\bf x})$ by applying the *Bayes rule* (generative models).\n",
    "> *Question*: How will you use the output of a probabilistic model, i.e., $p(y \\vert {\\bf x})$ for the actual task, i.e., classification or regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian vs. Non-Bayesian Probabilistic Models\n",
    "This is to do with *Bayesian* vs. *Frequentist* schools of thought in Statistics. In a typical non-Bayesian ML formulation, the unknown model parameters are learned/estimated by maximizing the log-likelihood of data, along with some other *prior* constraints on the solution.\n",
    "\n",
    "For instance, consider **Ordinary Linear Regression**:\n",
    "$$\n",
    "y = {\\bf w}^\\top{\\bf x} + \\epsilon\n",
    "$$\n",
    "or\n",
    "$$\n",
    "y \\sim \\mathcal{N}({\\bf w}^\\top {\\bf x}, \\sigma^2)\n",
    "$$\n",
    "The frequentist MLE approach gives the following estimate for ${\\bf w}$:\n",
    "$$\n",
    "{\\bf w}  =  ({\\bf X}^\\top{\\bf X})^{-1}{\\bf X}^\\top {\\bf y}\n",
    "$$\n",
    "\n",
    "While the *Bayesian* treatment of regression has the same formulation as above, the only difference is that the parameters, i.e., ${\\bf w}$ and $\\sigma^2$ are also treated as random variables. This means that we have a prior distribution of for the parameters, and we can calculate the posterior distribution for the parameters, after observing the data, using the Bayes rule.\n",
    "\n",
    "In general, that is the fundamental difference between the two types of approaches. In Bayesian methods, the model parameters are treated as random variables, and the observed data is used to revise the prior distribution for these parameters, to obtain a posterior distribution, i.e.,\n",
    "$$\n",
    "p({\\Theta}\\vert D) \\propto p(\\Theta) \\times p(D\\vert \\Theta)\n",
    "$$\n",
    "where $D$ denotes the observed (training) data. We will go over this in more detail in the first part of this course, for linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric vs. Non-parametric Methods\n",
    "\n",
    "Parameteric machine learning methods, as the name suggests, have parameters. Linear regression is a parametric method, because it has a parameter, the weight vector ${\\bf w}$. A neural network is a parametric method, too. \n",
    "\n",
    "A non-parametric method, does not have parameters. Which essentially means that it **does not make any assumptions about the form of the relationship between ${\\bf x}$ and $y$**. For instance, a $k$-nearest neighbor classifier is a non-parametric method. To classify a test instance, it uses the entire training data set to perform the classification. \n",
    "\n",
    "> A $k$-nearest neighbor does have a user-defined \"parameter\" - $k$. We will call such entities as *hyper-parameters* to differentiate them from the actual parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what is this course about?\n",
    "<div align=\"centered\">\n",
    "    <img src=\"images/mlmodels.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why?\n",
    "- Because not all problems can be solved by deep learning\n",
    "- How to solve problems with \"small data\"\n",
    "- Uncertainty?\n",
    "\n",
    "<img src=\"images/deepgp.png\" />\n",
    "          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
