{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ublogo.png\"/>\n",
    "\n",
    "### CSE610 - Bayesian Non-parametric Machine Learning\n",
    "\n",
    "  - Lecture Notes\n",
    "  - Instructor - Varun Chandola\n",
    "  - Term - Fall 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "The objective of this notebook is to discuss the relationship between Gaussian process based methods, discussed in the class so far, and deep-learning. In particular, we will talk about: a). the relationship between the two, b). how to combine GPs with neural networks, and c). how to make GPs behave like deep neural networks, i.e., deep Gaussian Processes.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** This material is based on a collection of recent papers on this topic, including [Deep neural networks as Gaussian Processes](https://arxiv.org/pdf/1711.00165.pdf), and [Deep Gaussian Processes](http://proceedings.mlr.press/v31/damianou13a.pdf).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivalence between GP and Neural Networks\n",
    "\n",
    "> This was established way back in 1994 by Radford Neal in this PhD thesis - *Bayesian Learning for Neural Networks*\n",
    "\n",
    "Consider a multi-layered perceptron. For any hidden layer (indexed by $l$), we will use the following notation:\n",
    "- $N_l$ - width of the layer $l$ or the number of nodes\n",
    "- ${\\bf x}^l$ - input to the $l^{th}$ layer (length will be $N_{l-1}$)\n",
    "- ${\\bf z}^l$ - output (before non-linear transformation) of the $l^{th}$ layer (length will be $N_l$) \n",
    "- ${\\bf b}^l$ - bias vector for the $l^{th}$ layer (length will be $N_l$)\n",
    "- $\\phi(\\cdot)$ - non-linear activation function\n",
    "- $i$ - index of the $i^{th}$ node in the hidden layer\n",
    "- $W^l$ - weight matrix at the $l^{th}$ hidden layer\n",
    "  * $W^{l}_{ij}$ - The $j^{th}$ component of the weight vector for the $i^{th}$ unit in the $l^{th}$ layer\n",
    "\n",
    "Thus, ${\\bf x}^0$ will correspond to the input (${\\bf x}$) to the neural network, while $\\phi({\\bf z}^L)$ will be the output of the neural network, where $L$ is the index of the final layer.\n",
    "\n",
    "Consider a single-hidden layer neural network. The input to the final layer will be, ${\\bf x}^1$, whose $j^{th}$ component will be:\n",
    "$$\n",
    "x_j^1({\\bf x}) = \\phi(b_j^0 + \\sum_{k=1}^d W^0_{jk}x_k)\n",
    "$$\n",
    "Notice that we are using $x_k$ (the $k^{th}$ entry in the input vector ${\\bf x}$), instead of the general term $x^0_k$.\n",
    "\n",
    "The output of the final layer will be:\n",
    "$$\n",
    "z_i^1({\\bf x}) = b_i^1 + \\sum_{j=1}^{N_0} W^1_{ij}x^1_j\n",
    "$$\n",
    "\n",
    "Also note that we are writing these terms as a function of the input $({\\bf x})$, since their values depend on the input to the neural network.\n",
    "\n",
    "We assume that the weight and bias parameters for layer $l$ are independent and randomly drawn from a Gaussian with zero mean and variance as $\\sigma^2_w/N_l$ and $\\sigma^2_b$, respectively.\n",
    "\n",
    "> **Observation 1**: The post-activations, $x^1_j({\\bf x})$ and $x^1_{j'}({\\bf x})$ are independent of each other. This is true because the weight and bias parameters are assumed to be i.i.d.\n",
    "\n",
    "> **Observation 2**: $z^1_i({\\bf x})$ will be Gaussian distributed. This is true because $z^1_i({\\bf x})$ is a sum of i.i.d. terms. And, the **Central Limit Theorem** dictates that in the limit of infinite width, i.e., $N_1 \\rightarrow \\infty$, the sum will be a random variable with a Gaussian distribution.\n",
    "\n",
    "Consider the values of $z^1_i$ for several inputs, $\\{{\\bf x}_1, {\\bf x}_2,\\ldots,{\\bf x}_k\\}$.\n",
    "> **Observation 3**: The finite collection, $\\{z^1_i({\\bf x}_1), z^1_i({\\bf x}_2),\\ldots,z^1_i({\\bf x}_k\\})$, will have a joint multivariate Gaussian distribution.\n",
    "\n",
    "If we consider $z^1_i(\\cdot)$ as a function of ${\\bf x}$, then the above observation is equivalent to a Gaussian process.\n",
    "\n",
    "> **Observation 4**: $z^1_i({\\bf x}) \\sim \\mathcal{GP}(\\mu^1({\\bf x}),k^1({\\bf x},{\\bf x}'))$\n",
    "\n",
    "Note that the GP prior is independent of $i$.\n",
    "\n",
    "Since the parameters have zero mean, the mean function will also be 0, i.e.,\n",
    "$$\n",
    "\\mu^1({\\bf x}) \\equiv \\mathbb{E}[z_i^1({\\bf x})] = 0\n",
    "$$\n",
    "The covariance function can also be derived from the above expressions as:\n",
    "$$\n",
    "k^1({\\bf x},{\\bf x}') \\equiv \\mathbb{E}[z_i^1({\\bf x})z_i^1({\\bf x}')] = \\sigma^2_b + \\sigma^2_w\\mathbb{E}[x_i^1({\\bf x})x_i^1({\\bf x}')] \\equiv \\sigma^2_b + \\sigma^2_wC({\\bf x},{\\bf x}')\n",
    "$$\n",
    "where $C(\\cdot,\\cdot)$ can be analytically computed for a given activation function $\\phi(\\cdot)$. \n",
    "\n",
    "For example, as discussed in Chapter 4 of the GPML book, if $\\phi(\\cdot)$ is the `erf` function, i.e.,\n",
    "$$\n",
    "\\phi(y) = erf(y) = \\frac{2}{\\pi}\\int_0^y e^{-t^2}dt\n",
    "$$\n",
    "then the corresponding $C$ function is obtained as:\n",
    "$$\n",
    "C({\\bf x},{\\bf x}') = \\frac{2}{\\pi}\\sin^{-1}\\left(\\frac{2\\tilde{{\\bf x}}^\\top\\Sigma \\tilde{{\\bf x}}'}{\\sqrt{(1+2\\tilde{{\\bf x}}^\\top\\Sigma \\tilde{{\\bf x}})(1+2\\tilde{{\\bf x}}'^\\top\\Sigma \\tilde{{\\bf x}}')}}\\right)\n",
    "$$\n",
    "where $\\tilde{\\bf x}'$ is the augmented input vector (including a 1 for the bias term) and $\\Sigma$ is a diagonal matrix consisting of $\\sigma^2_b$ and $\\sigma^2_w$ on the diagonal terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Above discussion shows that an infinite width single-hidden layer neural network is equivalent to a GP with a specific covariance function, which depends on the activation function used in the neural network.\n",
    "</div>\n",
    "\n",
    "This can be extended to deeper layers by induction. Thus, if the neural network has several hidden layers with infinite width, the output at each hidden layer is equivalent to a GP, where the covariance function at a given hidden layer recursively depends on the covariance function at the previous hidden layer.\n",
    "\n",
    "$$\n",
    "k^l({\\bf x},{\\bf x}') = \\sigma^2_b + \\sigma^2_wF_{\\phi}(k^{l-1}({\\bf x},{\\bf x}'),k^{l-1}({\\bf x},{\\bf x}),k^{l-1}({\\bf x}',{\\bf x}'))\n",
    "$$\n",
    "where $F_{\\phi}$ is a determinstic function whose form depends on the activation function $\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this get us?\n",
    "\n",
    "With the above equivalence, we can now construct a GP model, and use it as a infinite width neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/gpnn.png'/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
